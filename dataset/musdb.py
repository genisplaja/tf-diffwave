import os
import gc
import tqdm
import math

from glob import glob

import random
import librosa
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
from joblib import Parallel, delayed


class MUSDB:
    """MUSDB dataset loader.
    Use other opensource vocoder settings, 16bit, sr: 22050.
    """
    SR = 22050

    def __init__(self, config):
        """Initializer.
        Args:
            config: Config, dataset configuration.
            data_dir: str, dataset directory
                , defaults to '~/tensorflow_datasets'.
            download: bool, download dataset or not.
            from_tfds: bool, load from tfrecord generated by tfds or read raw audio.
        """
        self.config = config
        self.normalized = None

    @staticmethod
    def _load_audio(path):
        """Load audio with tf apis.
        Args:
            path: str, wavfile path to read.
        Returns:
            tf.Tensor, [T], mono audio in range (-1, 1).
        """
        raw = tf.io.read_file(path)
        audio, _ = tf.audio.decode_wav(raw, desired_channels=1)
        return tf.squeeze(audio, axis=-1)

    @staticmethod
    def _load_audio_librosa(path):
        """Load audio with tf apis.
        Args:
            path: str, wavfile path to read.
        Returns:
            tf.Tensor, [T], mono audio in range (-1, 1).
        """
        raw, _ = librosa.load(path=path, mono=True)
        return 2.*(raw - np.min(raw))/np.ptp(raw)-1 

    def normalizer(self, frames=16000):
        """Create LJSpeech normalizer, make fixed size segment in range(-1, 1).
        Args:
            frames: int, segment size, frame unit.
            from_tfds: bool, whether use tfds tfrecord or raw audio.
        Returns:
            Callable, normalizer.
        """
        def normalize(speech):
            """Normalize datum.
            Args:
                speech: tf.Tensor, [T], mono audio in range (-1, 1).
            Returns:
                tf.Tensor, [frames], fixed size speech signal in range (-1, 1). 
            """
            nonlocal frames
            frames = frames // self.config.hop * self.config.hop
            start = tf.random.uniform(
                (), 0, tf.shape(speech)[0] - frames, dtype=tf.int32)
            return speech[start:start + frames]

        return normalize

    def mel_fn(self, signal):
        """Generate log mel-spectrogram from input audio segment.
        Args:
            signal: tf.Tensor, [B, T], audio segment.
        Returns:
            tuple,
                signal: tf.Tensor, [B, T], identity to inputs.
                logmel: tf.Tensor, [B, T // hop, mel], log mel-spectrogram.
        """
        if tf.shape(signal)[1] < self.config.frames:
            signal_pad = tf.zeros([self.config.batch, tf.abs(self.config.frames-tf.shape(signal)[1])])
            signal_ok = tf.concat([signal, signal_pad], 0)
        else:
            signal_ok = signal

        padlen = self.config.win // 2
        # [B, T + win - 1]
        center_pad = tf.pad(signal_ok, [[0, 0], [padlen, padlen - 1]], mode='reflect')
        # [B, T // hop, fft // 2 + 1]
        stft = tf.signal.stft(
            center_pad,
            frame_length=self.config.win,
            frame_step=self.config.hop,
            fft_length=self.config.fft,
            window_fn=self.config.window_fn())

        return signal_ok, stft

    @staticmethod
    def complex_max(d):
        return d[np.unravel_index(np.argmax(np.abs(d), axis=None), d.shape)]

    @staticmethod
    def complex_min(d):
        return d[np.unravel_index(np.argmin(np.abs(d), axis=None), d.shape)]

    def normlize_complex(self, data, c_max=1):
        if c_max != 1:
            factor = np.divide(self.complex_max(data), c_max)
        else:
            factor = 1
        # normalize between 0-1
        output = np.divide((data - self.complex_min(data)),
                        (self.complex_max(data) - self.complex_min(data)))
        return np.multiply(output, factor)  # scale to the original range

    @staticmethod
    def check_shape(data):
        n = data.shape[0]
        if n % 2 != 0:
            n = data.shape[0] - 1
        return np.expand_dims(data[:n, :], axis=2)

    def get_max_complex(self, data, keys):
        pos = np.argmax([np.abs(self.complex_max(data[i])) for i in keys])
        return np.array([self.complex_max(data[i]) for i in keys])[pos]

    def load_a_file(self, fl):
        data = {}
        data_tmp = np.load(fl, allow_pickle=True)
        data = np.empty([*data_tmp['mixture'].shape], dtype=np.complex64)
        c_max = self.get_max_complex(data_tmp, ['mixture'])
        data[:, :] = self.normlize_complex(data_tmp['mixture'], c_max)
        audio_path = fl.replace('complex', 'raw_audio').replace('.npz', '/vocals.wav')
        audio = self._load_audio_librosa(audio_path)
        return fl.split('/')[-1].replace('.npz', ''), data, audio

    def batch_size(self, feat_list, features, is_audio=False):
        if is_audio is False:
            frame_size = self.config.unet_input_shape[1]
            for i in np.arange(math.floor(features.shape[1]/frame_size)):
                feat_list.append(self.check_shape(features[:, i*frame_size:(i+1)*frame_size]))
        else:
            frame_size = self.config.unet_input_shape[1] * self.config.hop
            for i in np.arange(math.floor(len(features)/frame_size)):
                feat_list.append(features[i*frame_size:(i+1)*frame_size])

        return feat_list

    def load_data(self, files):
        """The data is loaded in memory just once for the generator to have direct
        access to it"""
        data = {
            k: (v, a) for k, v, a in Parallel(n_jobs=16, verbose=5)(
                    delayed(self.load_a_file)(fl=fl) for fl in files
                )
        }
        _ = gc.collect()
        return data

    def get_data(self):
        path=self.config.path_spec
        data = self.load_data(random.sample(glob(os.path.join(path, '*.npz')), 5))
        train_tracks = random.sample([i for i in data.keys()], 4)
        validation_tracks = [i for i in data.keys() if i not in train_tracks]

        mixture_list, target_list = [], []
        for i in tqdm.tqdm(train_tracks):
            mixture_list = self.batch_size(mixture_list, data[i][0])
            target_list = self.batch_size(target_list,  data[i][1], is_audio=True)

        mixture_list_val, target_list_val = [], []
        for i in tqdm.tqdm(validation_tracks):
            mixture_list_val = self.batch_size(mixture_list_val, data[i][0])
            target_list_val = self.batch_size(target_list_val,  data[i][1], is_audio=True)

        dataset = {
            'train_mix': np.abs(mixture_list),
            'train_target': target_list, 
            'eval_mix': np.abs(mixture_list_val),
            'eval_target': target_list_val, 
        }

        return dataset

    @staticmethod
    def create_data_generator(dataset, batch_size, split):
        if split == 'train':
            mixture_list = dataset['train_mix']
            target_list = dataset['train_target']
        if split == 'eval':
            mixture_list = dataset['eval_mix']
            target_list = dataset['eval_target']
        length = len(target_list)
        idx = [i for i in range(length)]
        random.shuffle(idx)

        i = 0
        while True:

            if i + batch_size > length:
                i = 0
                random.shuffle(idx)

            mixtures, targets = [], []
            for j in range(i, i + batch_size):
                mixture = mixture_list[idx[j]]
                target = target_list[idx[j]]

                mixtures.append(mixture)
                targets.append(target)

            i += batch_size
            yield (np.stack(mixtures, axis=0), np.stack(targets, axis=0))