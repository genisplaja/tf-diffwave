import os
import glob

import librosa
import math
import numpy as np
import tensorflow as tf

class MUSDB_3SEC:
    """LJ Speech dataset loader.
    Use other opensource vocoder settings, 16bit, sr: 22050.
    """
    SR = 22050

    def __init__(self, config, data_dir=None):
        """Initializer.
        Args:
            config: Config, dataset configuration.
            data_dir: str, dataset directory
                , defaults to '~/tensorflow_datasets'.
            download: bool, download dataset or not.
            from_tfds: bool, load from tfrecord generated by tfds or read raw audio.
        """
        self.config = config
        self.rawset, self.info = self.load_data(data_dir)
        # [fft // 2 + 1, mel]
        #melfilter = librosa.filters.mel(
        #    config.sr, config.fft, config.mel, config.fmin, config.fmax).T
        #self.melfilter = tf.convert_to_tensor(melfilter)

        self.normalized = None

    def load_data(self, data_dir=None):
        """Load dataset from tfrecord or raw audio files.
        Args:
            data_dir: str, dataset directory.
                For from_tfds, None is acceptable
                 and set to default value '~/tensorflow_datasets'.
                For from raw audio, None is not acceptable.
        Returns:
            tf.data.Dataset, data loader.
        """
        mixture_files = glob.glob(os.path.join(data_dir, '*mixture.wav'))
        # generate file lists
        files = tf.data.Dataset.from_tensor_slices(
            [(mix, mix.replace('_mixture.', '_vocals.'), mix.replace('_mixture.', '_accompaniment.')) for mix in mixture_files])
        # read audio
        return files.map(MUSDB_3SEC._load_audio), None

    @staticmethod
    def _load_audio(paths):
        """Load audio with tf apis.
        Args:
            path: str, wavfile path to read.
        Returns:
            tf.Tensor, [T], mono audio in range (-1, 1).
        """
        mixture = tf.io.read_file(paths[0])
        vocals = tf.io.read_file(paths[1])
        accomp = tf.io.read_file(paths[2])
        mixture_audio, _ = tf.audio.decode_wav(mixture, desired_channels=1)
        vocal_audio, _ = tf.audio.decode_wav(vocals, desired_channels=1)
        accomp_audio, _ = tf.audio.decode_wav(accomp, desired_channels=1)
        return tf.squeeze(mixture_audio, axis=-1), tf.squeeze(vocal_audio, axis=-1), tf.squeeze(accomp_audio, axis=-1)

    def normalizer(self, frames=16000):
        """Create LJSpeech normalizer, make fixed size segment in range(-1, 1).
        Args:
            frames: int, segment size, frame unit.
            from_tfds: bool, whether use tfds tfrecord or raw audio.
        Returns:
            Callable, normalizer.
        """
        def normalize(mixture_signal, vocal_signal, accomp_signal):
            """Normalize datum.
            Args:
                speech: tf.Tensor, [T], mono audio in range (-1, 1).
            Returns:
                tf.Tensor, [frames], fixed size speech signal in range (-1, 1). 
            """
            nonlocal frames
            frames = frames // self.config.hop * self.config.hop
            start = tf.random.uniform(
                (), 0, tf.shape(mixture_signal)[0] - frames, dtype=tf.int32)
            return mixture_signal[start:start + frames], vocal_signal[start:start + frames], accomp_signal[start:start + frames]

        return normalize

    def mel_fn(self, mixture_signal, vocal_signal):
        """Generate log mel-spectrogram from input audio segment.
        Args:
            signal: tf.Tensor, [B, T, 2], audio segment.
        Returns:
            tuple,
                signal: tf.Tensor, [B, T], identity to inputs.
                logmel: tf.Tensor, [B, T // hop, mel], log mel-spectrogram.
        """
        padlen = self.config.win // 2
        # [B, T + win - 1]
        center_pad = tf.pad(mixture_signal, [[0, 0], [padlen, padlen - 1]], mode='reflect')
        # [B, T // hop, fft // 2 + 1]
        stft = tf.signal.stft(
            center_pad,
            frame_length=self.config.win,
            frame_step=self.config.hop,
            fft_length=self.config.fft,
            window_fn=self.config.window_fn())

        return tf.abs(self.check_shape(stft)), vocal_signal

    def mel_val(self, mixture_signal, vocal_signal):
        """Generate log mel-spectrogram from input audio segment.
        Args:
            signal: tf.Tensor, [B, T, 2], audio segment.
        Returns:
            tuple,
                signal: tf.Tensor, [B, T], identity to inputs.
                logmel: tf.Tensor, [B, T // hop, mel], log mel-spectrogram.
        """
        padlen = self.config.win // 2
        # [B, T + win - 1]
        center_pad = tf.pad(mixture_signal, [[0, 0], [padlen, padlen - 1]], mode='reflect')
        # [B, T // hop, fft // 2 + 1]
        stft = tf.signal.stft(
            center_pad,
            frame_length=self.config.win,
            frame_step=self.config.hop,
            fft_length=self.config.fft,
            window_fn=self.config.window_fn())

        # tf.abs(stft) --> magnitude spec of mixture, signal[1] --> vocal signal
        return tf.abs(self.check_shape(stft)), mixture_signal, vocal_signal

    def stft_batcher(self, audio_len=66150, input_shape=128):
        """Create LJSpeech normalizer, make fixed size segment in range(-1, 1).
        Args:
            frames: int, segment size, frame unit.
            from_tfds: bool, whether use tfds tfrecord or raw audio.
        Returns:
            Callable, normalizer.
        """
        def stft_batch(spec, mixture_signal, vocal_signal):
            """Normalize datum.
            Args:
                speech: tf.Tensor, [T], mono audio in range (-1, 1).
            Returns:
                tf.Tensor, [frames], fixed size speech signal in range (-1, 1). 
            """
            nonlocal audio_len, input_shape
            spec_shape = math.ceil(audio_len/self.config.hop)
            num_batches = math.floor(spec_shape/input_shape)
            new_audio_len = num_batches * input_shape * self.config.hop

            feat_list = []
            for i in np.arange(num_batches):
                feat_list.append(spec[:, i*input_shape:(i+1)*input_shape, :])
            return tf.stack(feat_list), mixture_signal[:new_audio_len], vocal_signal[:new_audio_len]
        
        return stft_batch
        
    @staticmethod
    def check_shape(data):
        n = data.shape[-1]
        if n % 2 != 0:
            n = data.shape[-1] - 1
        return data[:, :, :n]

    def dataset(self):
        """Generate dataset.
        """
        if self.normalized is None:
            self.normalized = self.rawset \
                .map(self.normalizer(self.config.frames)) \
                .batch(self.config.batch) \
                #.map(self.mel_fn)
        return self.normalized

    def validation(self):
        """Generate dataset.
        """
        return self.rawset \
            .map(self.normalizer(87040)) \
            .batch(1) \
            #.map(self.mel_val) \
            #.map(self.stft_batcher(66150, 128))